代码架构分为三个部分

第一个部分：
        1.获取代理，这部分主要在proxypool/crawler.py文件中实现，主要抓取四个免费ip代理的网站
        重点难点：
                1.获取到网页内容如何提取出想要的ip和端口内容：主要采用正则匹配的方式。

第二部分：
      2.测试代理，这部分在proxypool/tester.py中，使用aiohttp异步请求框架，测试每个连接是否可以用。
                不可用的进行减分，可以用的进行加分
        重点难点：
                aiphttp框架的使用

第三部分：
       3.提供api来取出代理，这部分在proxypool/api.py中，主要获取通过127.0.0.1:5000/random随机进行获取数据库中的代理
       重点难点：
              flask框架的使用


主函数 run();在proxypoll/scheduler.py中，通过开启多进程的方式同时开启爬取代理，测试代理，和开发api接口。


